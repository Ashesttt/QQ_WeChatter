from typing import Dict, List, Union, Any, Coroutine
from urllib.parse import quote
import re

import requests
from bs4 import BeautifulSoup
from loguru import logger

from wechatter.commands.handlers import command
from wechatter.commands.mcp import mcp_server
from wechatter.exceptions import Bs4ParsingError
from wechatter.models.wechat import SendTo
from wechatter.sender import sender
from wechatter.utils import get_request


@command(
    command="food-calories",
    keys=["é£Ÿç‰©çƒ­é‡", "food-calories", "çƒ­é‡", "calories", "å¡è·¯é‡Œ"],
    desc="è·å–é£Ÿç‰©çƒ­é‡ä¿¡æ¯ï¼ˆæ•°æ®æ¥æºï¼šå–µå’•ç¾é£Ÿï¼‰ã€‚",
)
async def food_calories_command_handler(to: Union[str, SendTo], message: str = "") -> None:
    try:
        result = get_food_calories_str(message)
    except Exception as e:
        error_message = f"è·å–é£Ÿç‰©çƒ­é‡å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}"
        logger.error(error_message)
        sender.send_msg(to, error_message)
    else:
        sender.send_msg(to, result)


@food_calories_command_handler.mainfunc
def get_food_calories_str(message: str) -> str:
    if not message:
        return "æŸ¥è¯¢å¤±è´¥ï¼Œè¯·è¾“å…¥é£Ÿç‰©åç§°"

    search_url = f"https://www.miaofoods.com/search/{quote(message)}.html"
    response = get_request(url=search_url)
    food_list = _parse_search_results(response)
    food_details = _get_food_details(food_list)
    return _generate_food_message(food_details)


def _get_food_details(food_list: List[Dict]) -> List[Dict]:
    """è·å–é£Ÿç‰©è¯¦æƒ…åˆ—è¡¨"""
    details = []
    for food in food_list[:5]:  # å–å‰5ä¸ªç»“æœ
        try:
            detail_url = f"https://www.miaofoods.com{food['path']}"
            response = get_request(url=detail_url)
            detail = _parse_detail_page(response)
            details.append(detail)
        except Exception as e:
            logger.warning(f"è·å–é£Ÿç‰©è¯¦æƒ…å¤±è´¥ï¼š{str(e)}")
            continue

    if not details:
        logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¥å…»ä¿¡æ¯")
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¥å…»ä¿¡æ¯")
    return details


def _generate_food_message(food_details: List[Dict]) -> str:
    """ç”Ÿæˆæ ¼å¼åŒ–æ¶ˆæ¯"""
    if not food_details:
        logger.error("é£Ÿç‰©è¯¦æƒ…åˆ—è¡¨ä¸ºç©º")
        raise ValueError("é£Ÿç‰©è¯¦æƒ…åˆ—è¡¨ä¸ºç©º")

    msg = "âœ¨=====é£Ÿç‰©åˆ—è¡¨=====âœ¨\n"
    for idx, item in enumerate(food_details, 1):
        msg += (
            f"{idx}. {item['name']}\n"
            f"   âœ… åˆ†ç±»ï¼š{item.get('category', 'N/A')}\n"
            f"   ğŸ”¥ çƒ­é‡ï¼š{item.get('calories', 'N/A')}kcal\n"
            f"   ğŸš ç¢³æ°´ï¼š{item.get('carbohydrate', 'N/A')}g\n"  # æ³¨æ„ä¿æŒkeyä¸€è‡´æ€§
            f"   ğŸ¥© è›‹ç™½è´¨ï¼š{item.get('protein', 'N/A')}g\n"
            f"   ğŸ§ˆ è„‚è‚ªï¼š{item.get('fat', 'N/A')}g\n"
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
        )
    return msg


def _parse_search_results(response: requests.Response) -> List[Dict]:
    """è§£ææœç´¢ç»“æœé¡µé¢"""
    soup = BeautifulSoup(response.text, "html.parser")
    script_tag = soup.find("script", string=lambda t: t and "window.__NUXT__" in t)

    if not script_tag:
        logger.error("æœªæ‰¾åˆ°åŒ…å« window.__NUXT__ çš„è„šæœ¬æ ‡ç­¾")
        raise Bs4ParsingError("è§£ææœç´¢ç»“æœå¤±è´¥")

    script_content = script_tag.string
    match = re.search(r"curSearchFoodList\s*:\s*\[(.*?)\](,|\})", script_content, re.DOTALL)

    if not match:
        logger.error("æœªæ‰¾åˆ° curSearchFoodList æ•°æ®")
        raise Bs4ParsingError("è§£æé£Ÿç‰©åˆ—è¡¨å¤±è´¥")

    food_list_str = match.group(1)
    food_tokens = re.findall(r'foodToken\s*:\s*"(.*?)"', food_list_str)

    return [{"path": f"/detail/{token}.html"} for token in food_tokens[:10]]


def _parse_detail_page(response: requests.Response) -> Dict:
    """è§£æè¯¦æƒ…é¡µè¥å…»ä¿¡æ¯"""
    soup = BeautifulSoup(response.text, "html.parser")
    nutrition = {}

    # è§£æåŸºç¡€ä¿¡æ¯
    info_div = soup.find("div", class_="food-detail-info")
    if info_div:
        for sub_div in info_div.find_all("div", class_="mtb-10"):
            text = sub_div.get_text(strip=True)
            if text.startswith("åç§°ï¼š"):
                name_span = sub_div.find("span")
                if name_span:
                    nutrition["name"] = name_span.get_text(strip=True)
            elif text.startswith("åˆ†ç±»ï¼š"):
                category_span = sub_div.find("span")
                if category_span:
                    nutrition["category"] = category_span.get_text(strip=True)

    # è§£æè¥å…»æˆåˆ†
    for div in soup.find_all("div", class_="food-detail-view"):
        spans = div.find_all("span")
        if len(spans) == 2:
            key = spans[0].get_text(strip=True)
            value = spans[1].get_text(strip=True)
            key_map = {
                "çƒ­é‡": "calories",
                "è„‚è‚ª": "fat",
                "è›‹ç™½è´¨": "protein",
                "ç¢³æ°´åŒ–åˆç‰©": "carbohydrate"
            }
            if key in key_map:
                nutrition[key_map[key]] = value

    # å¿…è¦å­—æ®µæ ¡éªŒ
    if "name" not in nutrition:
        logger.warning("æœªèƒ½è§£æåˆ°é£Ÿç‰©åç§°")
        raise ValueError("é£Ÿç‰©ä¿¡æ¯ä¸å®Œæ•´")

    return nutrition

@mcp_server.tool(
    name="get_food_calories",
    description="è·å–é£Ÿç‰©çƒ­é‡ä¿¡æ¯ï¼ˆæ•°æ®æ¥æºï¼šå–µå’•ç¾é£Ÿï¼‰ã€‚",
)
async def get_food_calories(message: str) -> Coroutine[Any, Any, str] | str:
    """
    è·å–é£Ÿç‰©ç›¸å…³ä¿¡æ¯ï¼ŒåŒ…æ‹¬é£Ÿç‰©çš„åˆ†ç±»ï¼Œçƒ­é‡ï¼Œç¢³æ°´ï¼Œè›‹ç™½è´¨ï¼Œè„‚è‚ª
    :param message: é£Ÿç‰©åç§°
    :return: é£Ÿç‰©ç›¸å…³ä¿¡æ¯
    """
    try:
        result = get_food_calories_str(message)
        return result
    except Exception as e:
        error_message = f"è·å–é£Ÿç‰©çƒ­é‡å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}"
        logger.error(error_message)
        return error_message

